---
title: 'Are Pre-trained Convolutions Better than Pre-trained Transformers?'
date: 2021-07-15
permalink: /are_pre-trained_convolutions_better_than_pre-trained_transformers
tags:
  - NLP
---

这是一篇来自ACL 2021的文章,  不同于以往Transformers征战CV无人可敌的情况, CV届传统一哥CNN现今也杀回NLP了领域.  Google Research团队通过**Convolutions替代Transformer**进行**预训练微调范式(pre-train-fine-tune paradigm)**, 在8个NLP任务中取得了不亚于Transformer的结果. 文章也对广大巨无霸预训练模型(GPT3 etc)的成功得益与pre-training schemes或是model architectures提出了以下三个疑问, 让我们带着问题来阅读这篇文章. 

> 1. Are only Transformers able to capitalize on the benefits of pre-training? (只有Transformers才能发挥出预训练范式的优点吗?)
> 2. If we use a different architectural inductive bias, would there also be a substantial gain unlocked by pre-training? (如果借助不同的模型架构作为归纳偏差进行预训练, 是否也会有显著收益?)
> 3. Are pretrained convolutions better in particular scenarios? (在特定场景下，预训练卷积是否会比Transformers更好?)

Paper Title: *Are Pre-trained Convolutions Better than Pre-trained Transformers?*

Paper Source: ACL 2021

Paper Link: [https://arxiv.org/abs/2105.03322](https://arxiv.org/abs/2105.03322)

## 背景

####  Convolution-based models的优缺点

1. Self-attention需要O(N^2)的空间复杂度, 需要消耗大量的GPU空间. 基于这个问题改进方案包括Weight Sharing Quantization / Mixed Precision, Knowledge Distillation等. 第一作者的另外一篇文章[Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)中总结了相关模型与方法, 如图:

   ![image-20210715174238162](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210715174238162.png)

2. 卷积是局部操作的，不依赖于位置编码作为模型的顺序信号. 

3. 卷积也有不少令人遗憾的缺点. 比如, 不能访问全局信息意味着没法建立多个序列之间的cross-attention

#### CNN in NLP 

[[EMNLP 2014] Convolutional Neural Networks for Sentence Classification (TextCNN)](https://arxiv.org/abs/1408.5882)

[[ICML 2017] Convolutional sequence to sequence learning](https://arxiv.org/abs/1705.03122)

[[arXiv 2018] An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271)

[[ICLR 2019 Oral]Pay less attention with lightweight and dynamic convolutions](https://arxiv.org/abs/1901.10430)

## 模型架构

####  Lightweight Depthwise Convolution

**Depthwise Convolutions**: 基于channel axis的卷积, for $X \in \R^{n \times d}$, Depthwise Convolution $D\left(X, W_{c,:}, i, c\right)$具有和输如$X$相同的维度, 其中position $i$, channel $c$ 上的值$O_{i, c}$计算方式为:
$$
O_{i, c}=\sum_{j-1}^{k} W_{c, j} \cdot X_{i+j-\left[\frac{k+1}{2}\right]}, c
$$

**Lightweight Convolutions**: 借助softmax-normalized kernels来实现:

$$
O_{i, c}^{L}=\sum_{j-1}^{k} \operatorname{softmax}\left(W_{\hat{c}, j}\right) \cdot X_{\left.i+j-\left\lceil\frac{k+1}{2}\right\rceil\right)}, \hat{c}
$$

其中$\hat{c}=\frac{c H}{d}$, 即每$\frac{d}{H}$个output channels间参数是共享的. 当$H=1$时等价于所有channels的参数是共享的.

**Dynamic Convolutions**: 是Lightweight Convolutions的一种新形式,  其中$f(\cdot)$是一个线性变换用来学习position dependent kernel

$$
D_{Y}=L\left(X, f\left(X_{i}\right)_{h,:}, i, c\right)
$$

#### Span-based Seq2Seq pre-training

#### Pre-training Object

与BERT类似, 本文构造了一种基于Span预测的预训练目标: 将input sentence中的部分spans随机mask(repalce with special token [mask])并生成被mask的span, such as:

>*Inputs*: The happy cat sat [mask]
>
>*Outputs*: on the mat

####  Convolutional Seq2Seq Architecture

提出了一个预训练的卷积Seq2Seq模型，用卷积块替代了Transformer架构中的multi-headed selfattention机制, 其中每个卷积块可以表示如下:

$$
\begin{aligned}
&X^{1}=W^{I} X \odot \operatorname{sigmoid}\left(W^{S} X\right) \\
&X^{2}=\operatorname{ConvBlock}\left(X^{2}\right) \\
&X^{3}=W^{O}\left(X^{2}\right)
\end{aligned}
$$

与Transformer类似的, 本文提出的Convolutional Seq2Seq在每个Conv层的上下用Layer normalization和residual connectors包裹, 即:

$$
\begin{aligned}
&X_{A}=\operatorname{LayerNorm}(\operatorname{Conv}(X))+X, \\
&X_{B}=\operatorname{LayerNorm}\left(\mathrm{FFN}\left(X_{A}\right)+X_{A}\right.
\end{aligned}
$$

#### Optimization

基于token-wise cross-entropy loss可以得到模型最后的优化目标, 即对比input sequence和predicted sequence中每一个time step $t$的在不同class $i$下的结果

$$
L=\sum_{t=1}^{L} \sum_{i=1}^{n} \log \left(\pi_{i}^{t}\right)+\left(1-y_{i}^{t}\right) \log \left(1-\pi_{i}^{t}\right)
$$

## 实验与分析

#### 实验设置

在八种不同NLP任务下进行实验,  包括

1. Toxicity Detection: [CIVIL COMMENTS](https://arxiv.org/abs/1903.04561), [WIKI TOXIC SUBTYPES](https://doi.org/10.1145/3038912.3052591)
2. Sentiment Classification: [IMDb review](https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf), [Stanford Sentiment Treebank (SST-2)](https://aclanthology.org/D13-1170/), [Twitter Sentiment140(S140)](https://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf)
3. News Classification: [AGNews](https://arxiv.org/abs/1509.01626)
4. Question Classification: [TREC](https://www.aclweb.org/anthology/C02-1150)
5. Semantic Parsing / Compositional Generalization: [COGS](https://arxiv.org/abs/2010.05465)

具体数据集统计信息如下:

![image-20210715201821831](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210715201821831.png)

在实验过程中, seq2seq架构中的ConvBlock采用了多种卷积机制进行试验, 包括Lightweight convolutions, dynamic convolutions and dilated convolutions. 设置细节见原文. 同时对于**both raw (no pre-training) and pre-train-finetune paradigms**进行了对比试验, 预训练语料来自于[Colossal Cleaned CommonCrawl Corpus (C4)](https://www.tensorflow.org/datasets/catalog/c4), 最大掩盖span数量为3, 掩盖比例为15%.

#### 实验结果

![image-20210715202516615](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210715202516615.png)

以上实验结果表明:

1. 在7个数据集中(COGS没结果???)未经预训练的卷积要优于未经预训练的Transformers

2. 6/7项任务(除了IMDb)，经过预训练的卷积优于预训练的Transformers
3. 预训练微调范式(pre-train-fine-tune paradigm)同样可以帮助convolutions-based model提升模型性能, 并不是只有transformer-based model可以从中受益
4. 在预训练的卷积模型中, dilated convolutions $ \approx$ dynamic convolutions are generally $>$ lightweight convolutions
5. 模型在raw (non-pre-training)和pre-training训练下在下游任务的性能表现并没有直接关系. 告诫我们预训练微调范式固然可以结合不同模型架构使用, 但是还需要注意不同模型架构在预训练下的行为可能不同.

## 讨论与展望

**交叉注意力缺失**

由于卷积的结构特性, 其天然在交叉注意力归纳偏差(cross-attention inductive bias)的处理上有一定的缺失, 所以使用pre-trained convolutions处理自然语言推理(NLI)这类包含多序列语义交互的任务会显然易见地比较棘手. 作者在[SQuAD](https://arxiv.org/abs/1606.05250)以及[MultiNLI](https://arxiv.org/abs/1704.05426)这两个任务上对比了convolutions以及transformer进行了实验, 证明了这一猜测, 即premise和hypothesis, question和context缺少了cross-attention所带来的交互. 但是如果给Convolution-based encoder中加上一层简单的attention, 就可以达到与Transformer近似的结果.

|                                    | F1 on SQuAD | Accuracy on MultiNLI |
| ---------------------------------- | ----------- | -------------------- |
| Convolutions                       | **70%**     | 75%                  |
| Transformer                        | 90%         | **84%**              |
| Convolutions + one layer attention | 83%         | 83%                  |

**卷积可以更快更好地应对长序列**

在不同长度的预训练任务中, 卷积始终表现出了更快的运行速度, 且面对长序列场景Convolutions具有更好的可拓展性.

![image-20210715211820640](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210715211820640.png)

**卷积具有高效计算性**

FLOPS数量变化的对比上, 卷积也相对于Transformers表现出了优势—在所有序列长度上，卷积都更高效.

![image-20210715211810211](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210715211810211.png)

**卷积是否可以完全取代Tranformers**

虽然Transformers在NLP领域仍然是最主流的模型架构,  但本篇论文证实了卷积在模型质量、速度、FLOPs和可扩展性等方面都有其值得讨论的价值.  本文将预训练微调应用于卷积网络, 在某些任务下表表现出了Transformer相当甚至更优的性能. 不过卷积对于cross-attention的缺失仍然值得关注与探讨, 也是一个比较好的方向.

**预训练和模型架构优势不可混为一谈**

在当前的研究领域中, 大型预训练模型(BERT, GPT3, etc)一直与Transformers架构高度耦合. 但是预训练的优势其实和模型架构的优势应该被独立看待, 对于模型架构和预训练微调范式的各式花式组合的仍有很大的提高空间.